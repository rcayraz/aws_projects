# Amazon S3 Advanced Features

**Autor:** Roberto Ayra  
**Fecha:** Abril 2024

---

## ğŸ“Š MOVIMIENTO ENTRE CLASES DE ALMACENAMIENTO

Dependiendo de los requerimientos de acceso y costo, podemos mover los archivos entre distintos tipos de S3:

```
Standard â†’ Standard-IA â†’ One Zone-IA â†’ Glacier â†’ Glacier Deep Archive
   ğŸ’°ğŸ’°ğŸ’°        ğŸ’°ğŸ’°           ğŸ’°         ğŸ’°/2        ğŸ’°/5
  (Acceso        (Acceso       (Acceso    (Acceso     (Acceso
  frecuente)     ocasional)    ocasional) archivo)    archivo)
```

### ğŸ“‹ Recomendaciones por Tipo de Uso:

- **Objetos con acceso poco frecuente** â†’ **Standard-IA**
- **Objetos de archivo (backup/compliance)** â†’ **Glacier o Glacier Deep Archive**
- **Datos crÃ­ticos con acceso ocasional** â†’ **Standard-IA**
- **Datos no crÃ­ticos con acceso ocasional** â†’ **One Zone-IA**

---

## âš™ï¸ REGLAS DE CICLO DE VIDA (Lifecycle Rules)

### ğŸ”„ Acciones de TransiciÃ³n
Configuran objetos para que pasen automÃ¡ticamente a otra clase de almacenamiento:

```
DÃ­a 0: Standard
    â†“ (30 dÃ­as mÃ­nimo)
DÃ­a 30: Standard-IA
    â†“ (30 dÃ­as mÃ­nimo)
DÃ­a 60: One Zone-IA
    â†“ (90 dÃ­as mÃ­nimo desde Standard)
DÃ­a 90: Glacier Instant Retrieval
    â†“
DÃ­a 180: Glacier Flexible Retrieval
    â†“
DÃ­a 365: Glacier Deep Archive
```

### ğŸ—‘ï¸ Acciones de ExpiraciÃ³n
Configuran objetos para que se eliminen automÃ¡ticamente:

- **Logs de acceso**: Eliminar despuÃ©s de 365 dÃ­as
- **Versiones antiguas**: Eliminar versiones no actuales despuÃ©s de 30 dÃ­as
- **Uploads incompletos**: Eliminar multipart uploads despuÃ©s de 7 dÃ­as

### ğŸ“ Casos PrÃ¡cticos de Lifecycle Rules

#### **Caso 1: Empresa de Media y Entretenimiento**
**SituaciÃ³n**: Videos de streaming que se acceden frecuentemente los primeros 30 dÃ­as, ocasionalmente hasta 6 meses, y luego se archivan.

**SoluciÃ³n**:
```json
{
  "Rules": [{
    "Id": "MediaLifecycle",
    "Status": "Enabled",
    "Filter": {"Prefix": "videos/"},
    "Transitions": [
      {
        "Days": 30,
        "StorageClass": "STANDARD_IA"
      },
      {
        "Days": 180,
        "StorageClass": "GLACIER"
      }
    ]
  }]
}
```

#### **Caso 2: Logs de AplicaciÃ³n**
**SituaciÃ³n**: Logs que necesitan estar disponibles por 90 dÃ­as para anÃ¡lisis, luego archivados por compliance durante 7 aÃ±os.

**SoluciÃ³n**:
```json
{
  "Rules": [{
    "Id": "LogsLifecycle",
    "Status": "Enabled",
    "Filter": {"Prefix": "logs/"},
    "Transitions": [
      {
        "Days": 90,
        "StorageClass": "GLACIER"
      },
      {
        "Days": 365,
        "StorageClass": "DEEP_ARCHIVE"
      }
    ],
    "Expiration": {
      "Days": 2555
    }
  }]
}
``` 

---

## ğŸ“ˆ AMAZON S3 ANALYTICS

Herramienta que ayuda a tomar decisiones informadas sobre cuÃ¡ndo mover objetos a la clase de almacenamiento mÃ¡s eficiente en costos.

### ğŸ¯ CaracterÃ­sticas Principales:

- **AnÃ¡lisis de patrones de acceso** para Standard e Standard-IA
- **No incluye**: One Zone-IA ni clases Glacier
- **ActualizaciÃ³n**: Informes diarios
- **Tiempo inicial**: 24-48 horas para ver los primeros datos
- **Beneficio**: Base sÃ³lida para crear Lifecycle Rules

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   S3 Analytics  â”‚â”€â”€â”€â–¶â”‚  AnÃ¡lisis de     â”‚â”€â”€â”€â–¶â”‚  Lifecycle      â”‚
â”‚   Monitoring    â”‚    â”‚  Patrones de     â”‚    â”‚  Rules          â”‚
â”‚                 â”‚    â”‚  Acceso          â”‚    â”‚  Optimization   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’³ MODELO "REQUESTER PAYS" (El Solicitante Paga)

### ğŸ›ï¸ Modelo Tradicional:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    ğŸ’° Costos    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Propietario    â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   AWS S3        â”‚
â”‚  del Bucket     â”‚   â€¢ Storage     â”‚   Bucket        â”‚
â”‚                 â”‚   â€¢ Requests    â”‚                 â”‚
â”‚                 â”‚   â€¢ Data Transferâ”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–²                                   â–²
        â”‚                                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€ Acceso â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”„ Modelo "Requester Pays":
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Propietario    â”‚    ğŸ’° Storage   â”‚   AWS S3        â”‚
â”‚  del Bucket     â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   Bucket        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â–²
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    ğŸ’° Requests          â”‚
â”‚   Solicitante   â”‚    ğŸ’° Data Transfer     â”‚
â”‚   (Usuario)     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“‹ Casos de Uso:
- **Compartir datasets grandes** con otras organizaciones
- **DistribuciÃ³n de contenido** donde el consumidor paga el acceso
- **ColaboraciÃ³n entre empresas** con datos sensibles al costo

**âš ï¸ Requisito**: El solicitante debe estar autenticado en AWS (no puede ser anÃ³nimo)

---

## ğŸ”” NOTIFICACIONES DE EVENTOS S3

### ğŸ“ Tipos de Eventos Principales:
- `s3:ObjectCreated:*` - Objeto creado
- `s3:ObjectRemoved:*` - Objeto eliminado
- `s3:ObjectRestore:*` - Objeto restaurado desde Glacier
- `s3:ObjectReplication:*` - Objeto replicado

### ğŸ¯ Filtros Disponibles:
- **Por extensiÃ³n**: `*.jpg`, `*.pdf`
- **Por prefijo**: `logs/`, `images/`
- **Por tamaÃ±o**: Objetos > 1MB
- **Metadatos personalizados**

### ğŸ—ï¸ Arquitectura BÃ¡sica para GeneraciÃ³n de Miniaturas:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    ğŸ”” Event     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    ğŸ“¤ Message    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   S3 Bucket â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚     SNS     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚     SQS     â”‚
â”‚  (Images)   â”‚   *.jpg        â”‚   Topic     â”‚                  â”‚   Queue     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   Upload       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                         â”‚
                                                                         â”‚ Poll
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    ğŸ“¥ Process   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    ğŸ“¤ Trigger    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
â”‚   S3 Bucket â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚   Lambda    â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   Lambda   â”‚
â”‚(Thumbnails) â”‚   Save Thumb    â”‚  Function   â”‚                  â”‚  Function  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“Š Caso PrÃ¡ctico: ReorganizaciÃ³n de Datos CDC

**SituaciÃ³n**: Archivos CDC llegando en tiempo real desde 20 bases de datos con 100 tablas cada una.

**Estructura Actual**:
```
bucket/
â”œâ”€â”€ db1/
â”‚   â”œâ”€â”€ table1/files.parquet
â”‚   â”œâ”€â”€ table2/files.parquet
â”‚   â””â”€â”€ ...
â”œâ”€â”€ db2/
â”‚   â”œâ”€â”€ table1/files.parquet
â”‚   â””â”€â”€ ...
â””â”€â”€ db20/...
```

**Estructura Objetivo**:
```
analytics-bucket/
â”œâ”€â”€ table1/
â”‚   â”œâ”€â”€ db1/files.parquet
â”‚   â”œâ”€â”€ db2/files.parquet
â”‚   â””â”€â”€ ...
â”œâ”€â”€ table2/
â”‚   â””â”€â”€ ...
â””â”€â”€ table100/...
```

**Arquitectura de SoluciÃ³n**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    ğŸ”” S3 Event      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Source S3     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚      SQS        â”‚
â”‚   CDC Bucket    â”‚   ObjectCreated     â”‚   Dead Letter   â”‚
â”‚ db*/table*/*.   â”‚                     â”‚     Queue       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                       â–²
         â”‚                                       â”‚ Failed Messages
         â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                              â”‚   Lambda DLQ    â”‚
         â”‚                              â”‚   Processor     â”‚
         â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Read Object
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    ğŸ“¤ Reorganized   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Lambda        â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚  Target S3      â”‚
â”‚   Function      â”‚    File Structure   â”‚ Analytics Bucketâ”‚
â”‚ â€¢ Parse path    â”‚                     â”‚ table*/db*/     â”‚
â”‚ â€¢ Copy file     â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ â€¢ Reorganize    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**CÃ³digo Lambda (Python)**:
```python
import boto3
import json
from urllib.parse import unquote_plus

def lambda_handler(event, context):
    s3 = boto3.client('s3')
    
    for record in event['Records']:
        # Parse S3 event
        bucket = record['s3']['bucket']['name']
        key = unquote_plus(record['s3']['object']['key'])
        
        # Extract db and table from path: db1/table1/file.parquet
        path_parts = key.split('/')
        db_name = path_parts[0]  # db1
        table_name = path_parts[1]  # table1
        file_name = path_parts[2]  # file.parquet
        
        # New structure: table1/db1/file.parquet
        new_key = f"{table_name}/{db_name}/{file_name}"
        
        # Copy to new structure
        copy_source = {'Bucket': bucket, 'Key': key}
        s3.copy_object(
            CopySource=copy_source,
            Bucket='analytics-bucket',
            Key=new_key
        )
        
    return {'statusCode': 200}
```

### ğŸŒ‰ IntegraciÃ³n con EventBridge

**Caso**: Sistema de procesamiento de documentos legales con mÃºltiples workflows.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    ğŸ”” Event        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   S3 Bucket     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚   EventBridge   â”‚
â”‚  Legal Docs     â”‚   .pdf Upload      â”‚   Custom Bus    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚                 â”‚                 â”‚
                              â–¼                 â–¼                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Lambda    â”‚    â”‚Step Functionsâ”‚    â”‚   Kinesis   â”‚
                    â”‚ OCR Process â”‚    â”‚ Approval     â”‚    â”‚  Analytics  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ Workflow     â”‚    â”‚   Stream    â”‚
                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ventajas de EventBridge**:
- **Filtrado avanzado** con reglas JSON complejas
- **MÃºltiples destinos** simultÃ¡neos
- **Reintentos automÃ¡ticos** y DLQ
- **TransformaciÃ³n de eventos**

---

## ğŸš€ RENDIMIENTO Y OPTIMIZACIÃ“N S3

### ğŸ“Š MÃ©tricas de Rendimiento Base:
- **Escalabilidad**: AutomÃ¡tica para altas tasas de peticiÃ³n
- **Latencia**: 100-200 ms
- **Throughput por prefijo**:
  - **3,500** peticiones PUT/COPY/POST/DELETE por segundo
  - **5,500** peticiones GET/HEAD por segundo

### ğŸ“ OptimizaciÃ³n por Prefijos:

```
Bucket Structure:
â”œâ”€â”€ /logs/2024/01/    â†’ 5,500 GET/s
â”œâ”€â”€ /logs/2024/02/    â†’ 5,500 GET/s  
â”œâ”€â”€ /images/products/ â†’ 5,500 GET/s
â””â”€â”€ /videos/stream/   â†’ 5,500 GET/s
                        â•â•â•â•â•â•â•â•â•â•â•
                       22,000 GET/s Total
```

**ğŸ’¡ Consejo**: Distribuye las cargas entre mÃºltiples prefijos para maximizar el rendimiento.

---

## âš¡ TÃ‰CNICAS DE OPTIMIZACIÃ“N DE PERFORMANCE

### ğŸ”„ Multipart Upload

```
Archivo Grande (>100MB)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     10GB Video File        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ Split
            â–¼
â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”
â”‚Part1â”‚ â”‚Part2â”‚ â”‚Part3â”‚ â”‚Part4â”‚
â”‚ 2GB â”‚ â”‚ 2GB â”‚ â”‚ 2GB â”‚ â”‚ 4GB â”‚
â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜
   â”‚       â”‚       â”‚       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”˜
          Upload en Paralelo
   â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
   â”‚      S3 Bucket        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ğŸ“‹ Beneficios**:
- **Obligatorio**: Archivos > 5GB
- **Recomendado**: Archivos > 100MB
- **ParalelizaciÃ³n**: MÃºltiples parts simultÃ¡neamente
- **Resiliencia**: Reintentar solo parts que fallan

### ğŸŒ S3 Transfer Acceleration

```
Cliente (Madrid)
     â”‚ Upload
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Optimized     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Madrid    â”‚    Network       â”‚   S3 Bucket â”‚
â”‚ Edge Loc.   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ us-east-1   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    (AWS Backbone)â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ğŸ¯ Casos de Uso**:
- **Usuarios globales** subiendo a bucket en regiÃ³n especÃ­fica
- **Archivos grandes** con conexiones lentas
- **Compatible** con Multipart Upload

### ğŸ“¤ Byte-Range Fetches (RecuperaciÃ³n por Rangos)

#### Para Acelerar Descargas:
```
Archivo 100MB en S3
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  0-25MB â”‚ 25-50MB â”‚ 50-75MB â”‚75-100MB â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚         â”‚         â”‚         â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         Peticiones Paralelas
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Cliente                      â”‚
â”‚        Download 4x Faster               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Para Datos Parciales:
```
Log File 1GB
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Header    â”‚       Log Data           â”‚
â”‚  (1KB)     â”‚        (999MB)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â””â”€â”€ GET Range: bytes=0-1023
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Solo Header (1KB)                â”‚
â”‚      Ahorro 99.9% Transferencia         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” S3 SELECT y GLACIER SELECT

### ğŸ’¡ Concepto: "Server-Side Filtering"

```
Archivo CSV 100MB
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ timestamp,user_id,action,ip_address     â”‚
â”‚ 2024-01-01,123,login,192.168.1.1       â”‚
â”‚ 2024-01-01,456,logout,10.0.0.1         â”‚
â”‚ ... (millones de registros)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ S3 SELECT Query:
         â”‚ SELECT user_id, action FROM s3object 
         â”‚ WHERE timestamp LIKE '2024-01-01%'
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Resultado 2MB                   â”‚
â”‚ user_id,action                          â”‚
â”‚ 123,login                               â”‚
â”‚ 456,logout                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ğŸ’° Beneficios**:
- **98% menos transferencia** de datos
- **Menor costo de red**
- **Menos CPU** en el cliente
- **Soporte**: CSV, JSON, Parquet

### ğŸ“‹ Caso PrÃ¡ctico: AnÃ¡lisis de Logs
```python
import boto3

s3 = boto3.client('s3')

response = s3.select_object_content(
    Bucket='logs-bucket',
    Key='access-logs/2024-01-01.csv',
    Expression="""
        SELECT ip_address, COUNT(*) as requests 
        FROM s3object 
        WHERE status_code = '404' 
        GROUP BY ip_address
    """,
    ExpressionType='SQL',
    InputSerialization={
        'CSV': {'FileHeaderInfo': 'USE'},
        'CompressionType': 'GZIP'
    },
    OutputSerialization={'CSV': {}}
)
```

---

## ğŸ”„ S3 BATCH OPERATIONS

### ğŸ¯ Operaciones Masivas Disponibles:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           S3 Batch Operations           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Copiar objetos entre buckets          â”‚
â”‚ â€¢ Modificar metadatos/propiedades       â”‚
â”‚ â€¢ Cambiar storage classes               â”‚
â”‚ â€¢ Aplicar/modificar ACLs                â”‚
â”‚ â€¢ AÃ±adir/modificar tags                 â”‚
â”‚ â€¢ Cifrar objetos no cifrados            â”‚
â”‚ â€¢ Restaurar desde Glacier               â”‚
â”‚ â€¢ Invocar Lambda (custom actions)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ—ï¸ Arquitectura de Batch Operations:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    ğŸ“‹ Lista        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  S3 Inventory   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚   Batch Job     â”‚
â”‚   Report        â”‚   de Objetos       â”‚   Definition    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                      â”‚
         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
         â””â–¶â”‚   S3 Select     â”‚                  â”‚
           â”‚   Filtrado      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         ğŸ“‹ Lista Filtrada
                                             â”‚
                                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    ğŸ”„ Ejecuta     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Report      â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  Batch Job      â”‚
â”‚  â€¢ Success      â”‚   en Lotes        â”‚  Execution      â”‚
â”‚  â€¢ Failed       â”‚                   â”‚  â€¢ Reintentos   â”‚
â”‚  â€¢ Manifest     â”‚                   â”‚  â€¢ Progreso     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚  â€¢ Notificacionesâ”‚
                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“‹ Caso PrÃ¡ctico: MigraciÃ³n de Storage Class

**SituaciÃ³n**: Migrar 10 millones de objetos de Standard a Standard-IA en un bucket de 100TB.

**SoluciÃ³n**:
```json
{
  "JobId": "batch-migration-2024",
  "Description": "Migrate old objects to Standard-IA",
  "Manifest": {
    "Spec": {
      "Format": "S3InventoryReport_CSV_20161130"
    },
    "Location": {
      "ObjectArn": "arn:aws:s3:::inventory-bucket/2024/inventory.csv",
      "ETag": "example-etag"
    }
  },
  "Operation": {
    "S3PutObjectCopy": {
      "StorageClass": "STANDARD_IA",
      "MetadataDirective": "COPY"
    }
  },
  "Priority": 10,
  "RoleArn": "arn:aws:iam::123456789012:role/batch-operations-role"
}
```

**ğŸ’¼ Beneficios del Caso**:
- **Sin cÃ³digo**: No necesitas escribir Lambda
- **Manejo de errores**: AutomÃ¡tico con reportes
- **Progreso**: Seguimiento en tiempo real
- **Costo**: Solo pagas por las operaciones ejecutadas

---

## ğŸ“š RECOMENDACIONES PARA EL EXAMEN AWS

### ğŸ¯ Temas Clave que Aparecen Frecuentemente:

#### ğŸ“‹ **Lifecycle Rules y Storage Classes**
- **MÃ­nimos obligatorios**: 30 dÃ­as en Standard antes de IA, 90 dÃ­as antes de Glacier
- **Pregunta tÃ­pica**: "Â¿CuÃ¡l es la configuraciÃ³n mÃ¡s costo-efectiva para datos que se acceden semanalmente por 2 meses y luego nunca?"
  - **Respuesta**: Standard â†’ Standard-IA (30 dÃ­as) â†’ Glacier (90 dÃ­as)

#### ğŸ”” **S3 Events y Arquitecturas**
- **Escenarios comunes**: Procesamiento automÃ¡tico de archivos subidos
- **Pregunta tÃ­pica**: "Necesitas procesar imÃ¡genes automÃ¡ticamente cuando se suben a S3"
  - **Respuesta**: S3 Event â†’ SNS â†’ SQS â†’ Lambda (patrÃ³n de fan-out)

#### âš¡ **Performance Optimization**
- **Multipart Upload**: Obligatorio >5GB, recomendado >100MB
- **Transfer Acceleration**: Para usuarios globales
- **Pregunta tÃ­pica**: "Â¿CÃ³mo optimizar uploads de 10GB desde Asia a bucket en us-east-1?"
  - **Respuesta**: Transfer Acceleration + Multipart Upload

#### ğŸ” **S3 Select vs Normal Transfer**
- **Pregunta tÃ­pica**: "Necesitas solo 10% de un archivo de 1GB regularmente"
  - **Respuesta**: S3 Select para reducir transferencia y costos

### âš ï¸ **Errores Comunes en el Examen**:

1. **Confundir Storage Classes**: One Zone-IA NO es para datos crÃ­ticos
2. **Lifecycle mÃ­nimos**: No se puede ir directo a Glacier desde Standard (mÃ­nimo 90 dÃ­as)
3. **Requester Pays**: El usuario DEBE estar autenticado
4. **S3 Analytics**: NO funciona con Glacier classes

### ğŸ¯ **Palabras Clave del Examen**:
- "Cost-effective" â†’ Lifecycle Rules
- "Automatically process" â†’ S3 Events
- "Global users" â†’ Transfer Acceleration
- "Large files" â†’ Multipart Upload
- "Partial data" â†’ S3 Select
- "Million objects" â†’ Batch Operations

### ğŸ“Š **Preguntas de Ejemplo TÃ­picas**:

**Pregunta 1**: Una empresa necesita almacenar logs de aplicaciÃ³n que se consultan frecuentemente durante los primeros 30 dÃ­as, ocasionalmente durante los siguientes 60 dÃ­as, y despuÃ©s solo para auditorÃ­as anuales. Â¿CuÃ¡l es la estrategia mÃ¡s costo-efectiva?

**Respuesta**: 
- Standard (30 dÃ­as) â†’ Standard-IA (60 dÃ­as) â†’ Glacier (despuÃ©s de 90 dÃ­as)
- Configurar expiraciÃ³n despuÃ©s de 7 aÃ±os si no se requiere retenciÃ³n permanente

**Pregunta 2**: Necesitas procesar automÃ¡ticamente videos subidos a S3 para generar diferentes resoluciones. El procesamiento puede fallar y necesita reintentos. Â¿CuÃ¡l es la mejor arquitectura?

**Respuesta**:
- S3 Event â†’ SQS (con DLQ) â†’ Lambda â†’ Step Functions
- Usar SQS para garantizar procesamiento y manejo de fallos

---

## ğŸ“– RECURSOS ADICIONALES

### ğŸ“š **DocumentaciÃ³n Oficial**:
- [S3 Storage Classes](https://docs.aws.amazon.com/s3/latest/userguide/storage-class-intro.html)
- [S3 Event Notifications](https://docs.aws.amazon.com/s3/latest/userguide/NotificationHowTo.html)
- [S3 Performance Guidelines](https://docs.aws.amazon.com/s3/latest/userguide/optimizing-performance.html)
- [S3 Batch Operations](https://docs.aws.amazon.com/s3/latest/userguide/batch-ops.html)

### ğŸ”— **Labs Recomendados**:
1. **Configurar Lifecycle Rules** para un escenario de media company
2. **Implementar S3 Events** con Lambda para procesamiento de imÃ¡genes
3. **Probar Transfer Acceleration** con archivos grandes
4. **Usar S3 Select** para anÃ¡lisis de logs
5. **Configurar Batch Operations** para migraciÃ³n masiva de storage classes

### ğŸ’¡ **Tips de Estudio**:
- **Practica** configurando Lifecycle Rules con diferentes escenarios
- **Entiende** las diferencias entre SNS, SQS y Lambda en eventos S3
- **Memoriza** los lÃ­mites de tiempo para Storage Classes
- **Experimenta** con S3 Select usando datos reales
- **FamiliarÃ­zate** con los casos de uso de cada tÃ©cnica de optimizaciÃ³n

---

## ğŸ‘¨â€ğŸ’» **Sobre el Autor**

**Roberto Ayra**  
*AWS Solutions Architect*  
ğŸ“… **Abril 2024**

DocumentaciÃ³n creada como parte del material de estudio para certificaciones AWS. Este contenido estÃ¡ basado en las mejores prÃ¡cticas oficiales de AWS y experiencias reales de implementaciÃ³n en proyectos empresariales.

### ğŸ¯ **Especialidades del Autor**:
- Arquitecturas serverless con S3 y Lambda
- OptimizaciÃ³n de costos en almacenamiento
- MigraciÃ³n de datos a gran escala
- ImplementaciÃ³n de data lakes en AWS

---

### ğŸ”– **Notas Finales**

Esta documentaciÃ³n cubre los aspectos avanzados de Amazon S3 que son fundamentales para:
- **AWS Solutions Architect Associate/Professional**
- **AWS Developer Associate** 
- **AWS SysOps Administrator**

**ğŸ“ RecomendaciÃ³n de Estudio**: Dedica especial atenciÃ³n a los casos prÃ¡cticos y arquitecturas presentadas, ya que el examen AWS evalÃºa principalmente la capacidad de aplicar estos conceptos en escenarios reales.

Para consultas o actualizaciones de este material, contactar al autor o consultar la documentaciÃ³n oficial de AWS que se actualiza constantemente.

---
*Ãšltima actualizaciÃ³n: Abril 2024*  
*VersiÃ³n: 2.0*